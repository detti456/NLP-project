{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Imports and installs"
      ],
      "metadata": {
        "id": "yskHM2aIYwaB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yd21ll-R-YUS"
      },
      "outputs": [],
      "source": [
        "!pip install langchain==0.1.9 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community"
      ],
      "metadata": {
        "id": "IJTPH-uyOEVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGabRfOWAq5G"
      },
      "outputs": [],
      "source": [
        "!pip install pydantic==1.10.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azlc7VLlDOXc"
      },
      "outputs": [],
      "source": [
        "!pip install wikiextractor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FI-5GPxcS10z"
      },
      "outputs": [],
      "source": [
        "!pip install ragatouille"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIw5bPCVOqAS"
      },
      "outputs": [],
      "source": [
        "!pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78foP0mAvlHi"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sr8zmweZvmYX"
      },
      "outputs": [],
      "source": [
        "!unzip nlp_proj.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "id": "z_aayZq6SzZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score"
      ],
      "metadata": {
        "id": "5tKfp5YEU8Bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from evaluate import load"
      ],
      "metadata": {
        "id": "D7mN5563ZgNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "hWSH6w1S94bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZbJSP2cUlAH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import datasets\n",
        "from datetime import datetime\n",
        "from typing import Optional\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_core.vectorstores import VectorStore\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import GenerationConfig, pipeline\n",
        "from huggingface_hub import InferenceClient\n",
        "from tqdm.auto import tqdm\n",
        "from langchain.vectorstores import Chroma\n",
        "from ragatouille import RAGPretrainedModel\n",
        "from langchain.schema.retriever import BaseRetriever\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2f-dCj8bGzh"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_dict(d):\n",
        "  s = \"\"\n",
        "  for k, v in d.items():\n",
        "    if isinstance(v, dict):\n",
        "      s += str(k) + \" \" + parse_dict(v)\n",
        "    if isinstance(v, list):\n",
        "      s += str(k) + \" \" + \" \".join(f\"{i}\" for i in v)\n",
        "    else:\n",
        "      s += str(k) + \" \" + str(v)\n",
        "  return s"
      ],
      "metadata": {
        "id": "c-Mp_Q_ocXJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4uKuQvqUswK"
      },
      "outputs": [],
      "source": [
        "def process_json(json_file):\n",
        "  data_array = []\n",
        "  with open(json_file) as f:\n",
        "    data_bag = json.load(f)\n",
        "    for data in data_bag:\n",
        "      data_array.append(Document(page_content=parse_dict(data['infobox']), metadata={'date_created': data['timestamp'][:10]}))\n",
        "  return data_array"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=128)"
      ],
      "metadata": {
        "id": "RfNc2YZJPxrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bj0hN09ZeB5I"
      },
      "outputs": [],
      "source": [
        "model_kwargs = model_kwargs = {'device':'cuda'}\n",
        "embeddings = HuggingFaceEmbeddings(model_kwargs = model_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = Chroma(persist_directory='/content/nlp_proj', embedding_function=embeddings)"
      ],
      "metadata": {
        "id": "nZYpNCHCQ-9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below is for first-time setup of the vectorstore - after it is saved, we just load it from persist directory"
      ],
      "metadata": {
        "id": "UDqPPJchZqTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"vectorstore = Chroma(persist_directory='/content/nlp_proj', embedding_function=embeddings)"
      ],
      "metadata": {
        "id": "bQUfh2-v6cex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWxaQxCtUEkF"
      },
      "outputs": [],
      "source": [
        "\"\"\"rootdir = ('/content/text')\n",
        "\n",
        "text_info = []\n",
        "for subdir, dirs, files in os.walk(rootdir):\n",
        "    for file in files:\n",
        "        filepath = subdir + os.sep + file\n",
        "        text_info = text_info + process_json(filepath)\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rootdir = '/content/drive/MyDrive/jsons'"
      ],
      "metadata": {
        "id": "tPNDfUxHTQDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for subdir, dirs, files in os.walk(rootdir):\n",
        "    for file in files:\n",
        "        print(\"Preparing file: \", file)\n",
        "        filepath = subdir + os.sep + file\n",
        "        text_data = process_json(filepath)\n",
        "        chunked_text_data = text_splitter.split_documents(text_data)\n",
        "        vectorstore.add_documents(documents=chunked_text_data)\n",
        "        vectorstore.persist()"
      ],
      "metadata": {
        "id": "clcYJS8dP6Cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retriever"
      ],
      "metadata": {
        "id": "_LMXjYt6Zy8U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctJ0ciml29Ik"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import VectorStore\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema.retriever import BaseRetriever\n",
        "from scipy.spatial.distance import cdist\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "class CustomVectorStoreRetriever(BaseRetriever):\n",
        "\n",
        "    def time_score(self, metadatas, query_t, v_mean, v_std):\n",
        "      alpha = 0.2\n",
        "      d_time = []\n",
        "      for x in metadatas:\n",
        "        if x is None:\n",
        "          d_time.append(time.mktime(datetime.datetime.strptime('1970-01-01' , \"%Y-%m-%d\").timetuple()))\n",
        "        else:\n",
        "          d_time.append(time.mktime(datetime.datetime.strptime(x['date_created'], \"%Y-%m-%d\").timetuple()))\n",
        "      q_time = time.mktime(datetime.datetime.strptime(query_t, \"%Y-%m-%d\").timetuple())\n",
        "      ts = np.array([alpha/(q_time - d_t) for d_t in d_time])\n",
        "      reg_ts = [((x - np.std(ts)) / np.mean(ts)) * v_std + v_mean for x in ts]\n",
        "      return reg_ts\n",
        "\n",
        "\n",
        "    def cosine_sim_score(self, query_vector, doc_vector):\n",
        "        # Calculate cosine similarity between the vectors\n",
        "\n",
        "        return 1. - cdist(np.array(query_vector).reshape(1, -1), np.array(doc_vector).reshape(1, -1), 'cosine')\n",
        "\n",
        "    vector_store: VectorStore\n",
        "    vector_temp_score_fn = time_score\n",
        "    query_date: str = '2025-01-12'\n",
        "    top_k: int\n",
        "    include_temp: bool = False\n",
        "\n",
        "    def _get_relevant_documents(self, query):\n",
        "        # Embed the query to get its vector representation\n",
        "\n",
        "        query_vector_id = embeddings.embed_query(query)\n",
        "\n",
        "        # List to store documents, their respective scores, and metadata\n",
        "        doc_scores = []\n",
        "\n",
        "        doc_embeddings = self.vector_store.get(include=['embeddings'])['embeddings']\n",
        "        # query_embedding = self.vector_store.get(ids=query_vector_id, include=['embeddings'])['embeddings']\n",
        "\n",
        "        doc_texts = self.vector_store.get()['documents']\n",
        "\n",
        "        metadatas = self.vector_store.get()['metadatas']\n",
        "\n",
        "        for i in range (0, len(doc_embeddings)):\n",
        "          doc_scores.append([doc_texts[i], self.cosine_sim_score(query_vector_id, doc_embeddings[i])[0][0]])\n",
        "\n",
        "        arr = np.array([x[1] for x in doc_scores])\n",
        "\n",
        "\n",
        "        if self.include_temp == True:\n",
        "\n",
        "          time_vals = self.time_score(metadatas, self.query_date, np.mean(arr), np.std(arr))\n",
        "\n",
        "          a = np.add(arr, time_vals)\n",
        "          for i in range (0, len(doc_embeddings)):\n",
        "            doc_scores[i][1] = a[i]\n",
        "\n",
        "        # Sort documents by the score in descending order\n",
        "        sorted_docs = sorted(doc_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Return the documents sorted by similarity\n",
        "        return [Document(page_content=doc) for doc, _ in sorted_docs[:self.top_k]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model"
      ],
      "metadata": {
        "id": "PCq8_36_Z24B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qf9WbET4I3lA"
      },
      "outputs": [],
      "source": [
        "prompt_template = \"\"\"\n",
        "<|system|></s>\n",
        "<|user|>\n",
        "Additional context:\n",
        "{context}\n",
        "---\n",
        "Here is the question you need to answer.\n",
        "\n",
        "Question: {question}\n",
        "</s>\n",
        "<|assistant|>\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6xYb8qUTyOK"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "def answer_question(question, model_name = \"google/flan-t5-large\", k = 1, prompt = prompt, reranker: Optional[RAGPretrainedModel] = None):\n",
        "\n",
        "  if model_name == \"google/flan-t5-large\":\n",
        "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "  gen_cfg = GenerationConfig.from_pretrained(model_name)\n",
        "\n",
        "  pipe=pipeline(\n",
        "      task=\"text2text-generation\",\n",
        "      model=model,\n",
        "      tokenizer=tokenizer,\n",
        "      generation_config=gen_cfg\n",
        "  )\n",
        "\n",
        "  llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "  chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=CustomVectorStoreRetriever(vector_store = vectorstore, top_k = 10, include_temp = True),\n",
        "    chain_type_kwargs={\"prompt\": prompt},\n",
        ")\n",
        "  result = chain.invoke(query)\n",
        "  return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DoTMzRPOHsk"
      },
      "source": [
        "Obtaining results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "questions_base = []\n",
        "f = open('test_TLQA.json')\n",
        "data = json.load(f)\n",
        "for i in data:\n",
        "    questions_base.append(i['question'])\n",
        "\n",
        "# Closing file\n",
        "f.close()"
      ],
      "metadata": {
        "id": "mrPuBQ7LW6Ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "for i in range(len(questions_base)):\n",
        "  query = questions_base[i]\n",
        "  result = answer_question(query)\n",
        "  print(result['result'].strip())\n",
        "  results.append(result['result'].strip())"
      ],
      "metadata": {
        "id": "hCUaBIC0WGTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = open('results_k10_yes.txt','w')\n",
        "for item in results:\n",
        "    file.write(item+\"\\n\")\n",
        "file.close()"
      ],
      "metadata": {
        "id": "xLqu0brNPtjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "actual_answers = []\n",
        "f = open('test_processed.json')\n",
        "data = json.load(f)\n",
        "for i in data:\n",
        "    actual_answers.append(i['output'])\n",
        "# Closing file\n",
        "f.close()"
      ],
      "metadata": {
        "id": "j3Lvdi62PbAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_answers = []\n",
        "with open('results_k10_yes.txt', 'r') as file:\n",
        "    # Read each line in the file\n",
        "    for line in file:\n",
        "        # Print each line\n",
        "        predicted_answers.append(line.strip())"
      ],
      "metadata": {
        "id": "LDKKip6iREBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation"
      ],
      "metadata": {
        "id": "rT5SGVL2aKzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_entities_and_timelines(output):\n",
        "    entities = []\n",
        "    for item in output.split(\",\"):\n",
        "        item = item.strip()\n",
        "        if \"(\" in item and \")\" in item:\n",
        "            entity, timeline = item.rsplit(\"(\", 1)\n",
        "            entities.append((entity.strip(), timeline.strip(\")\")))\n",
        "        else:\n",
        "            entities.append((item.strip(), None))\n",
        "    return entities"
      ],
      "metadata": {
        "id": "UJ6oX64eTKJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function for BLEU and ROUGE metrics\n",
        "class TLQAMetrics:\n",
        "    def evaluate_predictions(self, predictions, references):\n",
        "        \"\"\"Evaluate BLEU and ROUGE scores.\"\"\"\n",
        "        # Load metrics\n",
        "        bleu = load('bleu')\n",
        "        rouge = load('rouge')\n",
        "\n",
        "        # Compute BLEU and ROUGE\n",
        "        bleu_scores = bleu.compute(predictions=predictions, references=references)\n",
        "        rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
        "\n",
        "        return {\n",
        "            \"BLEU\": bleu_scores,\n",
        "            \"ROUGE\": rouge_scores,\n",
        "        }\n",
        "\n",
        "# Initialize BLEU/ROUGE evaluator\n",
        "metrics = TLQAMetrics()"
      ],
      "metadata": {
        "id": "hMYrbCjAUX-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_true_positives = 0\n",
        "total_false_positives = 0\n",
        "total_false_negatives = 0\n",
        "total_timeline_matches = 0\n",
        "total_timeline_mismatches = 0\n",
        "total_ground_truth_entities = 0\n",
        "\n",
        "# Lists to store sample-wise (macro) metrics\n",
        "sample_precisions = []\n",
        "sample_recalls = []\n",
        "sample_f1s = []\n",
        "timeline_matches = []\n",
        "timeline_mismatches = []\n",
        "\n",
        "# Lists for BLEU and ROUGE references and predictions\n",
        "references = []\n",
        "sample_predictions = []\n",
        "\n",
        "# Prepare output lines for sample-wise and global results\n",
        "macro_output_lines = []\n",
        "micro_output_lines = []\n",
        "\n",
        "\n",
        "for i in range (len(actual_answers)):\n",
        "    # Parse ground truth and prediction\n",
        "    ground_truth = set(parse_entities_and_timelines(actual_answers[i]))\n",
        "    predicted = set(parse_entities_and_timelines(predicted_answers[i]))\n",
        "\n",
        "    # Extract entities and timelines separately\n",
        "    ground_truth_entities = {entity for entity, _ in ground_truth}\n",
        "    predicted_entities = {entity for entity, _ in predicted}\n",
        "\n",
        "    # Add reference and prediction for BLEU/ROUGE\n",
        "    references.append(actual_answers[i])\n",
        "    sample_predictions.append(predicted_answers[i])\n",
        "\n",
        "    # Calculate matches\n",
        "    true_positives = ground_truth_entities & predicted_entities\n",
        "    false_positives = predicted_entities - ground_truth_entities\n",
        "    false_negatives = ground_truth_entities - predicted_entities\n",
        "\n",
        "    # Precision, recall, F1 for the current sample\n",
        "    precision = len(true_positives) / (len(true_positives) + len(false_positives)) if len(true_positives) + len(false_positives) > 0 else 0\n",
        "    recall = len(true_positives) / (len(true_positives) + len(false_negatives)) if len(true_positives) + len(false_negatives) > 0 else 0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
        "\n",
        "    # Append sample-wise metrics\n",
        "    sample_precisions.append(precision)\n",
        "    sample_recalls.append(recall)\n",
        "    sample_f1s.append(f1)\n",
        "\n",
        "    # Evaluate timelines for matched entities\n",
        "    sample_timeline_matches = 0\n",
        "    sample_timeline_mismatches = 0\n",
        "    for entity, timeline in ground_truth:\n",
        "        if entity in predicted_entities:\n",
        "            predicted_timeline = next((t for e, t in predicted if e == entity), None)\n",
        "            if timeline == predicted_timeline:\n",
        "                sample_timeline_matches += 1\n",
        "            else:\n",
        "                sample_timeline_mismatches += 1\n",
        "\n",
        "    timeline_matches.append(sample_timeline_matches)\n",
        "    timeline_mismatches.append(sample_timeline_mismatches)\n",
        "\n",
        "    # Aggregate metrics for micro-averaging\n",
        "    total_true_positives += len(true_positives)\n",
        "    total_false_positives += len(false_positives)\n",
        "    total_false_negatives += len(false_negatives)\n",
        "    total_timeline_matches += sample_timeline_matches\n",
        "    total_timeline_mismatches += sample_timeline_mismatches\n",
        "    total_ground_truth_entities += len(ground_truth_entities)\n",
        "\n",
        "    # Prepare sample-wise evaluation details for macro output\n",
        "    macro_output_lines.append(f\"Sample {i + 1}:\")\n",
        "    macro_output_lines.append(f\"Input: {questions_base[i]}\")\n",
        "    macro_output_lines.append(f\"Ground Truth: {ground_truth}\")\n",
        "    macro_output_lines.append(f\"Prediction: {predicted}\")\n",
        "    macro_output_lines.append(f\"Precision: {precision:.4f}\")\n",
        "    macro_output_lines.append(f\"Recall: {recall:.4f}\")\n",
        "    macro_output_lines.append(f\"F1-Score: {f1:.4f}\")\n",
        "    macro_output_lines.append(f\"Timeline Matches: {sample_timeline_matches}\")\n",
        "    macro_output_lines.append(f\"Timeline Mismatches: {sample_timeline_mismatches}\")\n",
        "    macro_output_lines.append(\"\")\n",
        "\n",
        "macro_precision = np.mean(sample_precisions)\n",
        "macro_recall = np.mean(sample_recalls)\n",
        "macro_f1 = np.mean(sample_f1s)\n",
        "\n",
        "# Calculate overall timeline accuracy for macro results\n",
        "macro_timeline_accuracy = sum(timeline_matches) / (sum(timeline_matches) + sum(timeline_mismatches)) if sum(timeline_matches) + sum(timeline_mismatches) > 0 else 0\n",
        "\n",
        "# Append macro-averaged metrics to macro output\n",
        "macro_output_lines.append(\"Global Macro Metrics:\")\n",
        "macro_output_lines.append(f\"Macro Precision (Entities): {macro_precision:.4f}\")\n",
        "macro_output_lines.append(f\"Macro Recall (Entities): {macro_recall:.4f}\")\n",
        "macro_output_lines.append(f\"Macro F1-Score (Entities): {macro_f1:.4f}\")\n",
        "macro_output_lines.append(f\"Macro Timeline Accuracy: {macro_timeline_accuracy:.4f}\")\n",
        "\n",
        "# Calculate global (micro) metrics\n",
        "micro_precision = total_true_positives / (total_true_positives + total_false_positives) if total_true_positives + total_false_positives > 0 else 0\n",
        "micro_recall = total_true_positives / (total_true_positives + total_false_negatives) if total_true_positives + total_false_negatives > 0 else 0\n",
        "micro_f1 = 2 * (micro_precision * micro_recall) / (micro_precision + micro_recall) if micro_precision + micro_recall > 0 else 0\n",
        "\n",
        "micro_timeline_accuracy = total_timeline_matches / (total_timeline_matches + total_timeline_mismatches) if total_timeline_matches + total_timeline_mismatches > 0 else 0\n",
        "completeness = total_true_positives / total_ground_truth_entities if total_ground_truth_entities > 0 else 0\n",
        "\n",
        "# Append global metrics to micro output\n",
        "micro_output_lines.append(\"Global Micro Metrics:\")\n",
        "micro_output_lines.append(f\"Micro Precision (Entities): {micro_precision:.4f}\")\n",
        "micro_output_lines.append(f\"Micro Recall (Entities): {micro_recall:.4f}\")\n",
        "micro_output_lines.append(f\"Micro F1-Score (Entities): {micro_f1:.4f}\")\n",
        "micro_output_lines.append(f\"Micro Timeline Accuracy: {micro_timeline_accuracy:.4f}\")\n",
        "micro_output_lines.append(f\"Completeness: {completeness:.4f}\")\n",
        "\n",
        "# Evaluate BLEU and ROUGE scores\n",
        "bleu_rouge_results = metrics.evaluate_predictions(sample_predictions, references)\n",
        "\n",
        "# Append BLEU and ROUGE to macro output\n",
        "macro_output_lines.append(\"Global BLEU and ROUGE Metrics:\")\n",
        "macro_output_lines.append(f\"BLEU: {bleu_rouge_results['BLEU']}\")\n",
        "macro_output_lines.append(f\"ROUGE: {bleu_rouge_results['ROUGE']}\")\n",
        "\n",
        "# Save macro results to a file\n",
        "with open(\"k10_yes_evaluation_output_macro.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join(macro_output_lines))\n",
        "\n",
        "# Save micro results to a separate file\n",
        "with open(\"k10_yes_evaluation_output_micro.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join(micro_output_lines))\n",
        "\n",
        "print(\"Evaluation results saved to 'evaluation_output_macro.txt' and 'evaluation_output_micro.txt'.\")"
      ],
      "metadata": {
        "id": "XQOr9-riTXpO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}